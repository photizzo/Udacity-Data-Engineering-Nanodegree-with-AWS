# Udacity Data Engineering with AWS Nanodegree

Design data models, build data warehouses and data lakes, automate data pipelines, and manage massive datasets. 

* Create user-friendly Relational and NoSQL data models
* Create scalable and efficient data warehouses
* Work efficiently with massive datasets
* Build and interact with a cloud-based data lake
* Automate and monitor data pipelines
* Develop proficiency in Spark, Airflow, and AWS tools


## [Course 1 - Data Modeling](./Course%201-Data%20Modeling/) 
Create **Relational** and **NoSQL** data models to fit the diverse needs of data consumers. Use ETL to build databases in **PostgreSQL** and **Apache Cassandra**.

### Lessons
1. Introduction to Data Modeling 
2. Relational Data Models
3. NoSQL Data Models

### Projects
* [Project 1.1 - Data Modeling with Postgres](./Course%201-Data%20Modeling/Project%201.1-Data%20Modeling%20with%20Postgres/)
* [Project 1.2 - Data Modeling with Apache Cassandra](./Course%201-Data%20Modeling/Project%201.2-Data%20Modeling%20with%20Apache%20Cassandra/)


## [Course 2 - Cloud Data Warehouses](./Course%202-Cloud%20Data%20Warehouses/)
Create cloud-based data warehouses. Sharpen data warehousing skills, deepen understanding of data infrastructure, and be introduced to data engineering on the cloud using `Amazon Web Services (AWS)`.

### Lessons
1. Introduction to Data Warehouses
2. ELT and Data Warehouse Technology in the Cloud 
3. AWS Data Technologies
4. Implementing Data Warehouses on AWS

### Project
* [Project 2 - Cloud Data Warehouse](./Course%202-Cloud%20Data%20Warehouses/Project%202-Data%20Warehouse/)


## [Course 3 - Spark & Data Lakes](./Course%203-Spark%20and%20Data%20Lakes/)
Build a data lake on AWS and a data catalog following the principles of data lakehouse architecture. Learn about the big data ecosystem and the power of Apache Spark for data wrangling and transformation. Work with AWS data tools and services to extract, load, process, query, and transform semi-structured data in data lakes.

### Lessons
1. Big Data Ecosystem, Data Lakes, & Spark
2. Spark Essentials
3. Using Spark & Data Lakes in the AWS Cloud
4. Ingesting & organizing data in lakehouse architecture on AWS

### Project
* [Project 3 - STEDI Human Balance Analytics](./Course%203-Spark%20and%20Data%20Lakes/Project%203-STEDI%20Human%20Balance%20Analytics/)


## [Course 4 - Automoate Data Pipelines](./Course%204-Automate%20Data%20Pipelines/)
Dive into the concept of data pipelines. 
* Focus on applying the data pipeline concepts learn through Apache Airflow - concepts covered including data validation, DAGs, and Airflow. 
* Venture into AWS quality concepts like copying S3 data, connections and hooks, and Redshift Serverless. 
* Explore data quality through data lineage, data pipeline schedules, and data partitioning. 
* Put data pipelines into production by extending Airflow with plugins, implementing task boundaries, and refactoring DAGs. 

### Lessons
1. Data Pipelines
2. Airflow & AWS
3. Data Quality
4. Production Data Pipelines


### Project
* Project 4 - Data Pipelines with Airflow


[Program Syllabus](./Data%2BEngineering%2BNanodegree%2BProgram%2BSyllabus.pdf), more information about this program can be found by visiting [Udacity Data Engineering ND](https://www.udacity.com/course/data-engineer-nanodegree--nd027).
